{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08beedd-b219-4499-9bc7-dfc51008d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ai2-olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c790b8-b033-4664-8084-e87e19a6a0eb",
   "metadata": {},
   "source": [
    "# OLMO SFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d57619a6-cbc6-4019-9821-d66b18ca53cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da13205fd30b424f8ccf095a4d18cad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3b7897f95a4c359efaad5c07453b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dd7a1f1f874ad0a16f7dc07c0c5f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/huggingface_hub/file_download.py:1006: UserWarning: Not enough free disk space to download the file. The expected file size is: 9945.78 MB. The target location /home/jnainani_umass_edu/.cache/huggingface/hub only has 7977.57 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/huggingface_hub/file_download.py:1006: UserWarning: Not enough free disk space to download the file. The expected file size is: 9945.78 MB. The target location /home/jnainani_umass_edu/.cache/huggingface/hub/models--allenai--OLMo-7B-SFT/blobs only has 7977.57 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e1442b90bd4f14996c009da648a480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e48db203968416eb42f80e3438f4eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd61014215314c92a166f3ef4f41d8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f14e23fc9384b2689c68bca144d6253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea7da3189024e0fae59ecf4d68ee9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0652c94091d940ddb5bbd196f99b7f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/66.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is language modeling?\n",
      "<|assistant|>\n",
      "Language modeling is a type of artificial intelligence that is used to predict the most likely next word or phrase in a sequence of words. It is often used to improve the accuracy of natural language processing tasks such as speech recognition and language translation.\n"
     ]
    }
   ],
   "source": [
    "from hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\n",
    "olmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-7B-SFT\")\n",
    "tokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-7B-SFT\")\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is language modeling?\" },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# optional verifying cuda\n",
    "# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n",
    "# olmo = olmo.to('cuda')\n",
    "response = olmo.generate(input_ids=inputs.to(olmo.device), max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760b75c-f52a-4a79-8fa8-ff4dc7b78e7c",
   "metadata": {},
   "source": [
    "# OLMO Instruct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73507eb-0c62-406c-a2b0-c597bb2b11a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5671e557698b419d99e7cd9deebd0e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84d600823454860aada8911a8978296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/11.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfde93220854a1390a50d44a68b91d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d457068d48494e86c93e8057e242f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236a2eacdce74b5c973ee0d40a6c996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b59795cc2254893b67410e0e3325902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2176f6dd7f470db7112c00b6112e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3261743788554b31b5183558f1965e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22815f8b70d4fffae0cc148d4262460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/66.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is language modeling?\n",
      "<|assistant|>\n",
      "Language modeling is a type of natural language processing (NLP) task or machine learning (ML) algorithm that involves predicting the probability of a given sequence of words in a sentence or text. The goal of language modeling is to estimate the likelihood that a particular sentence or text sequence occurs in a corpus of text.\n",
      "\n",
      "In language modeling, a model calculates the probability of a word based on the context in which it appears. For example, the probability of a word appearing after a particular preceding word will\n"
     ]
    }
   ],
   "source": [
    "from hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\n",
    "olmo = OLMoForCausalLM.from_pretrained(\"allenai/OLMo-7B-Instruct\")\n",
    "tokenizer = OLMoTokenizerFast.from_pretrained(\"allenai/OLMo-7B-Instruct\")\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is language modeling?\" },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# optional verifying cuda\n",
    "# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n",
    "# olmo = olmo.to('cuda')\n",
    "response = olmo.generate(input_ids=inputs.to(olmo.device), max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbef4a0-74f3-4c83-a280-09d11f3c8ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
